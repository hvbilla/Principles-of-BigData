2019-09-03 20:10:23,148 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = projectH/10.0.2.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_121
************************************************************/
2019-09-03 20:10:23,161 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-03 20:10:23,593 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/core-site.xml
2019-09-03 20:10:23,740 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2019-09-03 20:10:23,866 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2019-09-03 20:10:24,138 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2019-09-03 20:10:24,450 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2019-09-03 20:10:24,457 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2019-09-03 20:10:24,465 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2019-09-03 20:10:24,516 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2019-09-03 20:10:24,518 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2019-09-03 20:10:24,518 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2019-09-03 20:10:24,548 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2019-09-03 20:10:24,552 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2019-09-03 20:10:24,553 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2019-09-03 20:10:24,555 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2019-09-03 20:10:24,635 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-03 20:10:24,709 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-03 20:10:24,709 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2019-09-03 20:10:24,726 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2019-09-03 20:10:24,734 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2019-09-03 20:10:24,737 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2019-09-03 20:10:24,739 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2019-09-03 20:10:24,740 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2019-09-03 20:10:24,745 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2019-09-03 20:10:24,843 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2019-09-03 20:10:24,843 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2019-09-03 20:10:24,846 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2019-09-03 20:10:24,846 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2019-09-03 20:10:24,853 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2019-09-03 20:10:24,853 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2019-09-03 20:10:24,855 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2019-09-03 20:10:24,855 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:10:24,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:10:24,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:10:24,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2019-09-03 20:10:24,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2019-09-03 20:10:24,872 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2019-09-03 20:10:24,872 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2019-09-03 20:10:24,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2019-09-03 20:10:24,889 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2019-09-03 20:10:24,889 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2019-09-03 20:10:24,889 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:10:24,890 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2019-09-03 20:10:24,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2019-09-03 20:10:24,894 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2019-09-03 20:10:24,894 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:10:24,894 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2019-09-03 20:10:24,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2019-09-03 20:10:24,895 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2019-09-03 20:10:24,930 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:10:24,947 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2019-09-03 20:10:24,965 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2019-09-03 20:10:24,965 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:10:24,967 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2019-09-03 20:10:25,021 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:10:25,027 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2019-09-03 20:10:25,035 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2019-09-03 20:10:25,035 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:10:25,036 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2019-09-03 20:10:25,121 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:10:25,122 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2019-09-03 20:10:25,124 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2019-09-03 20:10:25,124 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:10:25,125 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2019-09-03 20:10:25,145 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2019-09-03 20:10:25,229 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-03 20:10:25,237 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-03 20:10:25,245 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2019-09-03 20:10:25,255 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-03 20:10:25,259 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2019-09-03 20:10:25,259 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2019-09-03 20:10:25,259 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2019-09-03 20:10:25,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2019-09-03 20:10:25,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-03 20:10:25,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-03 20:10:25,264 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2019-09-03 20:10:25,264 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2019-09-03 20:10:25,859 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2019-09-03 20:10:25,862 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2019-09-03 20:10:25,863 INFO org.mortbay.log: jetty-6.1.26
2019-09-03 20:10:25,888 INFO org.mortbay.log: Extract jar:file:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /tmp/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2019-09-03 20:10:26,142 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:10:26,147 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2019-09-03 20:10:26,147 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:10:27,606 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2019-09-03 20:10:27,607 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2019-09-03 20:10:27,638 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:10:27,638 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2019-09-03 20:10:27,653 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2019-09-03 20:10:27,657 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:10:27,666 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2019-09-03 20:10:35,247 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved projectH to /default-rack
2019-09-03 20:10:35,263 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: projectH:46631 Node Transitioned from NEW to RUNNING
2019-09-03 20:10:35,264 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node projectH(cmPort: 46631 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId projectH:46631
2019-09-03 20:10:35,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node projectH:46631 clusterResource: <memory:8192, vCores:8>
2019-09-03 20:19:42,236 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2019-09-03 20:19:42,254 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2019-09-03 20:19:42,258 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2019-09-03 20:19:42,359 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2019-09-03 20:19:42,363 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2019-09-03 20:19:42,367 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-03 20:19:42,368 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2019-09-03 20:19:42,369 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2019-09-03 20:19:42,370 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-03 20:19:42,371 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2019-09-03 20:19:42,371 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2019-09-03 20:19:42,372 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2019-09-03 20:19:42,378 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2019-09-03 20:19:42,378 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2019-09-03 20:19:42,378 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-03 20:19:42,385 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2019-09-03 20:19:42,386 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-03 20:19:42,388 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2019-09-03 20:19:42,388 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException
2019-09-03 20:19:42,388 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2019-09-03 20:19:42,388 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2019-09-03 20:19:42,389 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2019-09-03 20:19:42,389 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2019-09-03 20:19:42,389 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2019-09-03 20:19:42,390 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2019-09-03 20:19:42,391 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2019-09-03 20:19:42,391 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2019-09-03 20:19:42,391 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2019-09-03 20:19:42,392 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2019-09-03 20:19:42,392 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at projectH/10.0.2.4
************************************************************/
2019-09-03 20:27:53,771 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = projectH/10.0.2.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/etc/hadoop:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/harsha/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_121
************************************************************/
2019-09-03 20:27:53,785 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-03 20:27:54,271 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/core-site.xml
2019-09-03 20:27:54,419 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2019-09-03 20:27:54,528 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2019-09-03 20:27:54,953 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2019-09-03 20:27:55,392 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2019-09-03 20:27:55,400 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2019-09-03 20:27:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2019-09-03 20:27:55,452 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2019-09-03 20:27:55,455 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2019-09-03 20:27:55,456 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2019-09-03 20:27:55,480 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2019-09-03 20:27:55,482 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2019-09-03 20:27:55,483 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2019-09-03 20:27:55,484 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2019-09-03 20:27:55,583 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-03 20:27:55,739 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-09-03 20:27:55,739 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2019-09-03 20:27:55,758 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2019-09-03 20:27:55,771 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2019-09-03 20:27:55,773 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2019-09-03 20:27:55,775 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2019-09-03 20:27:55,776 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2019-09-03 20:27:55,780 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/home/harsha/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2019-09-03 20:27:55,903 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2019-09-03 20:27:55,903 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2019-09-03 20:27:55,908 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2019-09-03 20:27:55,908 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2019-09-03 20:27:55,922 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2019-09-03 20:27:55,922 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2019-09-03 20:27:55,923 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2019-09-03 20:27:55,924 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:27:55,924 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:27:55,925 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2019-09-03 20:27:55,925 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2019-09-03 20:27:55,925 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2019-09-03 20:27:55,942 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2019-09-03 20:27:55,942 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2019-09-03 20:27:55,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2019-09-03 20:27:55,961 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2019-09-03 20:27:55,962 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2019-09-03 20:27:55,962 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:27:55,963 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2019-09-03 20:27:55,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2019-09-03 20:27:55,968 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2019-09-03 20:27:55,968 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:27:55,968 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2019-09-03 20:27:55,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2019-09-03 20:27:55,972 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2019-09-03 20:27:56,010 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:27:56,039 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2019-09-03 20:27:56,079 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2019-09-03 20:27:56,080 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:27:56,081 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2019-09-03 20:27:56,196 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:27:56,225 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2019-09-03 20:27:56,240 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2019-09-03 20:27:56,244 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:27:56,245 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2019-09-03 20:27:56,416 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:27:56,422 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2019-09-03 20:27:56,425 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2019-09-03 20:27:56,425 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:27:56,425 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2019-09-03 20:27:56,445 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2019-09-03 20:27:56,693 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-03 20:27:56,713 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-03 20:27:56,732 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2019-09-03 20:27:56,758 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-03 20:27:56,766 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2019-09-03 20:27:56,774 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2019-09-03 20:27:56,774 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2019-09-03 20:27:56,777 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2019-09-03 20:27:56,777 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-03 20:27:56,778 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-03 20:27:56,789 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2019-09-03 20:27:56,789 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2019-09-03 20:27:57,684 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2019-09-03 20:27:57,687 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2019-09-03 20:27:57,688 INFO org.mortbay.log: jetty-6.1.26
2019-09-03 20:27:57,740 INFO org.mortbay.log: Extract jar:file:/home/harsha/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /tmp/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2019-09-03 20:27:58,259 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:27:58,275 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2019-09-03 20:27:58,276 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2019-09-03 20:28:00,526 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2019-09-03 20:28:00,533 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2019-09-03 20:28:00,577 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-09-03 20:28:00,578 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2019-09-03 20:28:00,580 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2019-09-03 20:28:00,634 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2019-09-03 20:28:00,634 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-03 20:28:02,989 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved projectH to /default-rack
2019-09-03 20:28:02,992 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node projectH(cmPort: 43671 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId projectH:43671
2019-09-03 20:28:02,999 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: projectH:43671 Node Transitioned from NEW to RUNNING
2019-09-03 20:28:03,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node projectH:43671 clusterResource: <memory:8192, vCores:8>
2019-09-03 20:37:55,780 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2019-09-04 19:09:07,141 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2019-09-04 19:09:07,311 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2019-09-04 19:09:07,317 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2019-09-04 19:09:07,432 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2019-09-04 19:09:07,436 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-04 19:09:07,437 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2019-09-04 19:09:07,442 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2019-09-04 19:09:07,450 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2019-09-04 19:09:07,451 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2019-09-04 19:09:07,453 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-04 19:09:07,457 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2019-09-04 19:09:07,457 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2019-09-04 19:09:07,463 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2019-09-04 19:09:07,468 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2019-09-04 19:09:07,469 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-04 19:09:07,482 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2019-09-04 19:09:07,478 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException
2019-09-04 19:09:07,478 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2019-09-04 19:09:07,478 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2019-09-04 19:09:07,484 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2019-09-04 19:09:07,484 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2019-09-04 19:09:07,484 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2019-09-04 19:09:07,484 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2019-09-04 19:09:07,485 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2019-09-04 19:09:07,485 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2019-09-04 19:09:07,505 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2019-09-04 19:09:07,505 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2019-09-04 19:09:07,505 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2019-09-04 19:09:07,506 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2019-09-04 19:09:07,506 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at projectH/10.0.2.4
************************************************************/
2019-09-16 17:41:47,414 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 10
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 10 submitted by user harsha
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	IP=10.0.2.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1568146808723_0010
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1568146808723_0010
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from NEW to NEW_SAVING
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1568146808723_0010
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from NEW_SAVING to SUBMITTED
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1568146808723_0010 user: harsha leaf-queue of parent: root #applications: 1
2019-09-16 17:41:48,564 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1568146808723_0010 from user: harsha, in queue: default
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from SUBMITTED to ACCEPTED
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1568146808723_0010_000001
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from NEW to SUBMITTED
2019-09-16 17:41:48,568 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2019-09-16 17:41:48,568 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1568146808723_0010 from user: harsha activated in queue: default
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1568146808723_0010 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@34665768, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2019-09-16 17:41:48,568 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1568146808723_0010_000001 to scheduler from user harsha in queue default
2019-09-16 17:41:48,569 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from SUBMITTED to SCHEDULED
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000001 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000001
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0010_01_000001 of capacity <memory:2048, vCores:1> on host projectH:44737, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0010_000001 container=Container: [ContainerId: container_1568146808723_0010_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:41:49,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:41:49,172 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : projectH:44737 for container : container_1568146808723_0010_01_000001
2019-09-16 17:41:49,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:41:49,174 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1568146808723_0010_000001
2019-09-16 17:41:49,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1568146808723_0010 AttemptId: appattempt_1568146808723_0010_000001 MasterContainer: Container: [ContainerId: container_1568146808723_0010_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ]
2019-09-16 17:41:49,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from SCHEDULED to ALLOCATED_SAVING
2019-09-16 17:41:49,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from ALLOCATED_SAVING to ALLOCATED
2019-09-16 17:41:49,179 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1568146808723_0010_000001
2019-09-16 17:41:49,182 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1568146808723_0010_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] for AM appattempt_1568146808723_0010_000001
2019-09-16 17:41:49,182 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1568146808723_0010_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2019-09-16 17:41:49,182 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1568146808723_0010_000001
2019-09-16 17:41:49,182 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1568146808723_0010_000001
2019-09-16 17:41:49,222 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1568146808723_0010_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] for AM appattempt_1568146808723_0010_000001
2019-09-16 17:41:49,223 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from ALLOCATED to LAUNCHED
2019-09-16 17:41:50,177 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000001 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:41:55,123 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1568146808723_0010_000001 (auth:SIMPLE)
2019-09-16 17:41:55,134 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1568146808723_0010_000001
2019-09-16 17:41:55,134 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	IP=10.0.2.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1568146808723_0010	APPATTEMPTID=appattempt_1568146808723_0010_000001
2019-09-16 17:41:55,134 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from LAUNCHED to RUNNING
2019-09-16 17:41:55,134 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from ACCEPTED to RUNNING
2019-09-16 17:41:57,182 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000002 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:41:57,182 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000002
2019-09-16 17:41:57,182 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0010_01_000002 of capacity <memory:1024, vCores:1> on host projectH:44737, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2019-09-16 17:41:57,183 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0010_000001 container=Container: [ContainerId: container_1568146808723_0010_01_000002, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2019-09-16 17:41:57,183 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2019-09-16 17:41:57,183 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2019-09-16 17:41:57,210 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : projectH:44737 for container : container_1568146808723_0010_01_000002
2019-09-16 17:41:57,213 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:41:58,183 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000002 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:41:58,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1568146808723_0010
2019-09-16 17:42:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000002 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:42:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0010_01_000002 in state: COMPLETED event:FINISHED
2019-09-16 17:42:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000002
2019-09-16 17:42:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0010_01_000002 of capacity <memory:1024, vCores:1> on host projectH:44737, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2019-09-16 17:42:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=harsha user-resources=<memory:2048, vCores:1>
2019-09-16 17:42:01,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0010_01_000002, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2019-09-16 17:42:01,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:01,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:42:01,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0010_000001 released container container_1568146808723_0010_01_000002 on node: host: projectH:44737 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2019-09-16 17:42:03,163 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000003 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:42:03,163 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000003
2019-09-16 17:42:03,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0010_01_000003 of capacity <memory:1024, vCores:1> on host projectH:44737, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2019-09-16 17:42:03,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0010_000001 container=Container: [ContainerId: container_1568146808723_0010_01_000003, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2019-09-16 17:42:03,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2019-09-16 17:42:03,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:03,248 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:42:04,164 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000003 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:42:04,253 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1568146808723_0010
2019-09-16 17:42:07,333 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000003 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:42:07,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0010_01_000003 in state: COMPLETED event:FINISHED
2019-09-16 17:42:07,333 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000003
2019-09-16 17:42:07,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0010_01_000003 of capacity <memory:1024, vCores:1> on host projectH:44737, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2019-09-16 17:42:07,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=harsha user-resources=<memory:2048, vCores:1>
2019-09-16 17:42:07,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0010_01_000003, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2019-09-16 17:42:07,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:07,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:42:07,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0010_000001 released container container_1568146808723_0010_01_000003 on node: host: projectH:44737 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2019-09-16 17:42:07,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1568146808723_0010_000001 with final state: FINISHING, and exit status: -1000
2019-09-16 17:42:07,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from RUNNING to FINAL_SAVING
2019-09-16 17:42:07,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1568146808723_0010 with final state: FINISHING
2019-09-16 17:42:07,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from RUNNING to FINAL_SAVING
2019-09-16 17:42:07,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1568146808723_0010
2019-09-16 17:42:07,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from FINAL_SAVING to FINISHING
2019-09-16 17:42:07,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from FINAL_SAVING to FINISHING
2019-09-16 17:42:08,963 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1568146808723_0010 unregistered successfully. 
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0010_01_000001 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0010_01_000001 in state: COMPLETED event:FINISHED
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0010	CONTAINERID=container_1568146808723_0010_01_000001
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0010_01_000001 of capacity <memory:2048, vCores:1> on host projectH:44737, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=harsha user-resources=<memory:0, vCores:0>
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0010_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0010_000001 released container container_1568146808723_0010_01_000001 on node: host: projectH:44737 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2019-09-16 17:42:14,371 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1568146808723_0010_000001
2019-09-16 17:42:14,372 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1568146808723_0010_000001
2019-09-16 17:42:14,372 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0010_000001 State change from FINISHING to FINISHED
2019-09-16 17:42:14,372 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0010 State change from FINISHING to FINISHED
2019-09-16 17:42:14,372 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1568146808723_0010
2019-09-16 17:42:14,372 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1568146808723_0010,name=word count,user=harsha,queue=default,state=FINISHED,trackingUrl=http://projectH:8088/proxy/application_1568146808723_0010/,appMasterHost=projectH,startTime=1568655708563,finishTime=1568655727961,finalStatus=SUCCEEDED,memorySeconds=59953,vcoreSeconds=32,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2019-09-16 17:42:14,373 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1568146808723_0010_000001 is done. finalState=FINISHED
2019-09-16 17:42:14,373 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1568146808723_0010 requests cleared
2019-09-16 17:42:14,374 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1568146808723_0010 user: harsha queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2019-09-16 17:42:14,374 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1568146808723_0010 user: harsha leaf-queue of parent: root #applications: 0
2019-09-16 17:42:14,374 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1568146808723_0010_000001
2019-09-16 17:42:16,372 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2019-09-16 17:42:42,110 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 11
2019-09-16 17:42:43,549 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 11 submitted by user harsha
2019-09-16 17:42:43,549 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	IP=10.0.2.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1568146808723_0011
2019-09-16 17:42:43,549 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1568146808723_0011
2019-09-16 17:42:43,549 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from NEW to NEW_SAVING
2019-09-16 17:42:43,551 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1568146808723_0011
2019-09-16 17:42:43,552 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from NEW_SAVING to SUBMITTED
2019-09-16 17:42:43,553 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1568146808723_0011 user: harsha leaf-queue of parent: root #applications: 1
2019-09-16 17:42:43,553 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1568146808723_0011 from user: harsha, in queue: default
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from SUBMITTED to ACCEPTED
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1568146808723_0011_000001
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from NEW to SUBMITTED
2019-09-16 17:42:43,554 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2019-09-16 17:42:43,554 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1568146808723_0011 from user: harsha activated in queue: default
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1568146808723_0011 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@4226dd9b, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2019-09-16 17:42:43,554 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1568146808723_0011_000001 to scheduler from user harsha in queue default
2019-09-16 17:42:43,556 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from SUBMITTED to SCHEDULED
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000001 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000001
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0011_01_000001 of capacity <memory:2048, vCores:1> on host projectH:44737, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0011_000001 container=Container: [ContainerId: container_1568146808723_0011_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:42:44,394 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:44,395 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : projectH:44737 for container : container_1568146808723_0011_01_000001
2019-09-16 17:42:44,396 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:42:44,396 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1568146808723_0011_000001
2019-09-16 17:42:44,396 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1568146808723_0011 AttemptId: appattempt_1568146808723_0011_000001 MasterContainer: Container: [ContainerId: container_1568146808723_0011_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ]
2019-09-16 17:42:44,396 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from SCHEDULED to ALLOCATED_SAVING
2019-09-16 17:42:44,396 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from ALLOCATED_SAVING to ALLOCATED
2019-09-16 17:42:44,397 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1568146808723_0011_000001
2019-09-16 17:42:44,399 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1568146808723_0011_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] for AM appattempt_1568146808723_0011_000001
2019-09-16 17:42:44,399 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1568146808723_0011_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2019-09-16 17:42:44,399 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1568146808723_0011_000001
2019-09-16 17:42:44,399 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1568146808723_0011_000001
2019-09-16 17:42:44,423 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1568146808723_0011_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] for AM appattempt_1568146808723_0011_000001
2019-09-16 17:42:44,423 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from ALLOCATED to LAUNCHED
2019-09-16 17:42:45,395 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000001 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:42:50,488 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1568146808723_0011_000001 (auth:SIMPLE)
2019-09-16 17:42:50,496 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1568146808723_0011_000001
2019-09-16 17:42:50,496 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	IP=10.0.2.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1568146808723_0011	APPATTEMPTID=appattempt_1568146808723_0011_000001
2019-09-16 17:42:50,496 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from LAUNCHED to RUNNING
2019-09-16 17:42:50,496 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from ACCEPTED to RUNNING
2019-09-16 17:42:52,402 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000002 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:42:52,402 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000002
2019-09-16 17:42:52,402 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0011_01_000002 of capacity <memory:1024, vCores:1> on host projectH:44737, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2019-09-16 17:42:52,402 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0011_000001 container=Container: [ContainerId: container_1568146808723_0011_01_000002, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2019-09-16 17:42:52,402 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2019-09-16 17:42:52,403 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:52,548 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : projectH:44737 for container : container_1568146808723_0011_01_000002
2019-09-16 17:42:52,552 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:42:53,402 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000002 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:42:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1568146808723_0011
2019-09-16 17:42:56,413 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000002 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:42:56,413 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0011_01_000002 in state: COMPLETED event:FINISHED
2019-09-16 17:42:56,413 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000002
2019-09-16 17:42:56,413 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0011_01_000002 of capacity <memory:1024, vCores:1> on host projectH:44737, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2019-09-16 17:42:56,413 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=harsha user-resources=<memory:2048, vCores:1>
2019-09-16 17:42:56,414 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0011_01_000002, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2019-09-16 17:42:56,414 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:56,414 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:42:56,414 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0011_000001 released container container_1568146808723_0011_01_000002 on node: host: projectH:44737 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000003 Container Transitioned from NEW to ALLOCATED
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000003
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1568146808723_0011_01_000003 of capacity <memory:1024, vCores:1> on host projectH:44737, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1568146808723_0011_000001 container=Container: [ContainerId: container_1568146808723_0011_01_000003, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2019-09-16 17:42:58,416 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2019-09-16 17:42:58,623 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2019-09-16 17:42:59,418 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000003 Container Transitioned from ACQUIRED to RUNNING
2019-09-16 17:42:59,628 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1568146808723_0011
2019-09-16 17:43:02,686 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000003 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0011_01_000003 in state: COMPLETED event:FINISHED
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000003
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0011_01_000003 of capacity <memory:1024, vCores:1> on host projectH:44737, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=harsha user-resources=<memory:2048, vCores:1>
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0011_01_000003, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2019-09-16 17:43:02,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0011_000001 released container container_1568146808723_0011_01_000003 on node: host: projectH:44737 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2019-09-16 17:43:02,935 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1568146808723_0011_000001 with final state: FINISHING, and exit status: -1000
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from RUNNING to FINAL_SAVING
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1568146808723_0011 with final state: FINISHING
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from RUNNING to FINAL_SAVING
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1568146808723_0011
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from FINAL_SAVING to FINISHING
2019-09-16 17:43:02,936 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from FINAL_SAVING to FINISHING
2019-09-16 17:43:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1568146808723_0011 unregistered successfully. 
2019-09-16 17:43:09,332 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1568146808723_0011_01_000001 Container Transitioned from RUNNING to COMPLETED
2019-09-16 17:43:09,332 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1568146808723_0011_01_000001 in state: COMPLETED event:FINISHED
2019-09-16 17:43:09,332 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1568146808723_0011	CONTAINERID=container_1568146808723_0011_01_000001
2019-09-16 17:43:09,332 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1568146808723_0011_01_000001 of capacity <memory:2048, vCores:1> on host projectH:44737, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2019-09-16 17:43:09,332 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=harsha user-resources=<memory:0, vCores:0>
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1568146808723_0011_01_000001, NodeId: projectH:44737, NodeHttpAddress: projectH:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.2.4:44737 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1568146808723_0011_000001 released container container_1568146808723_0011_01_000001 on node: host: projectH:44737 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1568146808723_0011_000001
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1568146808723_0011_000001
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1568146808723_0011_000001 State change from FINISHING to FINISHED
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1568146808723_0011 State change from FINISHING to FINISHED
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1568146808723_0011_000001 is done. finalState=FINISHED
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=harsha	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1568146808723_0011
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1568146808723_0011 requests cleared
2019-09-16 17:43:09,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1568146808723_0011 user: harsha queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2019-09-16 17:43:09,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1568146808723_0011 user: harsha leaf-queue of parent: root #applications: 0
2019-09-16 17:43:09,334 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1568146808723_0011,name=word count,user=harsha,queue=default,state=FINISHED,trackingUrl=http://projectH:8088/proxy/application_1568146808723_0011/,appMasterHost=projectH,startTime=1568655763549,finishTime=1568655782936,finalStatus=SUCCEEDED,memorySeconds=59552,vcoreSeconds=32,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2019-09-16 17:43:09,334 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1568146808723_0011_000001
2019-09-16 17:43:11,335 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
